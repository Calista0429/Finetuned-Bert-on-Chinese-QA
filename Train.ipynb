{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd857246-0c56-417d-9199-6026af46abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from transformers import AdamW\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c0fa0a-e219-4c31-ab4e-1d817dfa79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "\ttorch.manual_seed(seed)\n",
    "\tif torch.cuda.is_available():\n",
    "\t\t\ttorch.cuda.manual_seed(seed)\n",
    "\t\t\ttorch.cuda.manual_seed_all(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "same_seeds(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4ebdc0-6f70-4b56-8928-e43c68527a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "D:\\miniconda\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\miniconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForQuestionAnswering,\n",
    ")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-chinese\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6c18e1-ac66-483a-b8a3-c062d634552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data[\"questions\"], data[\"paragraphs\"]\n",
    "\n",
    "train_questions, train_paragraphs = read_data(\"D://Code/Machine Translation/hw7_train.json\")\n",
    "dev_questions, dev_paragraphs = read_data(\"D://Code/Machine Translation/hw7_dev.json\")\n",
    "test_questions, test_paragraphs = read_data(\"D://Code/Machine Translation/hw7_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a509cee2-3cce-44bb-9515-a83ff076e9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 0,\n",
       "  'paragraph_id': 8164,\n",
       "  'question_text': '保定至西安的電報線路架設後約10年什麼建設開始營運？',\n",
       "  'answer_text': '盧漢鐵路盧保段',\n",
       "  'answer_start': 141,\n",
       "  'answer_end': 147},\n",
       " '2010年引入的廣州快速交通運輸系統是世界第二大快速運輸系統。每日載客量可達100萬人次。每小時的客流量峰值高達26,900名乘客，僅次於波哥大的快速交通系統。每10秒有一輛公共汽車，每輛公共汽車在一個方向上行駛350小時。該平台包括橋樑，是世界上最長的國家公共汽車快速運輸系統平台，長度為260米。目前，廣州市的出租車和公交車主要以液化石油氣為燃料，部分公交車採用油電，氣電混合技術。2012年底，一輛LNG燃料公共汽車開始啟動。2014年6月，引入了LNG插電式混合動力公交車取代LPG公交車。2007年1月16日，廣州市政府完全禁止在城市地區駕駛摩托車。違反禁令的機動車將被沒收。廣州市交通局聲稱，禁令的實施導致交通擁堵和車禍大大減少。廣州白雲國際機場位於白雲區與花都區交界處。它於2004年8月5日正式投入運營。它是中國第二繁忙的機場。機場取代了原先位於市中心的舊機場，無法滿足日益增長的航空需求。目前，機場有三個簡易機場，是中國第三個擁有三條跑道的民航機場。比2023年香港國際機場第三條跑道的預計完工時間提前了8年。')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[0], train_paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1afe705e-146b-426e-aef7-b22f8cda61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_q = []\n",
    "len_p = []\n",
    "for q in train_questions:\n",
    "    len_q.append(len(q[\"question_text\"]))\n",
    "for p in train_paragraphs:\n",
    "    len_p.append(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de8c97a-4a39-48cd-aad4-a44d7de3c22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(len_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a8c949-bcf8-4975-ab22-9d7ae4795cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437.5714658490443"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(len_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6621b0-6e24-400e-9503-f95efc050dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions and paragraphs separately\n",
    "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n",
    "\n",
    "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
    "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
    "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
    "\n",
    "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
    "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
    "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n",
    "\n",
    "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deedba55-c2b6-47c0-9215-f569f32a30a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=23, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions_tokenized[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e4a1ff-a00d-4d49-8312-439d43f0e442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=430, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paragraphs_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f0297b-7f4a-40c7-ba6b-8205fac6bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 60\n",
    "        self.max_paragraph_len = 150\n",
    "        \n",
    "        ##### TODO: Change value of doc_stride #####\n",
    "        # self.doc_stride = 150\n",
    "        self.doc_stride = 75\n",
    "\n",
    "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
    "\n",
    "        ##### TODO: Preprocessing #####\n",
    "        # Hint: How to prevent model from learning something it should not learn\n",
    "        if self.split == \"train\":\n",
    "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
    "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
    "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
    "\n",
    "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
    "            mid = (answer_start_token + answer_end_token) // 2\n",
    "            paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "            paragraph_end = paragraph_start + self.max_paragraph_len\n",
    "\n",
    "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
    "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n",
    "            \n",
    "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
    "            answer_start_token += len(input_ids_question) - paragraph_start\n",
    "            answer_end_token += len(input_ids_question) - paragraph_start\n",
    "            \n",
    "            # Pad sequence and obtain inputs to model \n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "            \n",
    "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "                \n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "                \n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "                \n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            \n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
    "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a54b02f-d72f-4a07-8ea4-bdbe2522a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, output):\n",
    "    ##### TODO: Postprocessing #####\n",
    "    # There is a bug and room for improvement in postprocessing \n",
    "    # Hint: Open your prediction file to see what is wrong \n",
    "    \n",
    "    answer = ''\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1]\n",
    "    \n",
    "    for k in range(num_of_windows):\n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
    "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
    "        \n",
    "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
    "        prob = start_prob + end_prob\n",
    "        \n",
    "        # Replace answer if calculated probability is larger than previous windows\n",
    "        #fix the bug \n",
    "        if start_index <= end_index:\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
    "                answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
    "        \n",
    "    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n",
    "    return answer.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b838d1-3806-4520-902e-62bb8c495abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    #save the checkpoint in wandb\n",
    "    savedir = \"./checkpoints/bert\",\n",
    "\n",
    "    # training epoch\n",
    "    num_epoch = 30,\n",
    "    \n",
    "    #Validate on validation set\n",
    "    validation = True,\n",
    "    \n",
    "    #print the loss per logging step\n",
    "    logging_step = 100,\n",
    "    \n",
    "    #learning rate\n",
    "    learning_rate = 1e-5,\n",
    "\n",
    "    # training batch size \n",
    "    train_batch_size = 8,\n",
    "\n",
    "    # logging\n",
    "    use_wandb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75ebc212-3ebc-4028-8bd4-04ead52d9889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "D:\\miniconda\\envs\\pytorch\\lib\\site-packages\\accelerate\\accelerator.py:382: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: baoxihuang0429 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Code\\wandb\\run-20250331_211732-ll0exe34</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert/runs/ll0exe34' target=\"_blank\">bert</a></strong> to <a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert' target=\"_blank\">https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert/runs/ll0exe34' target=\"_blank\">https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert/runs/ll0exe34</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\envs\\pytorch\\lib\\site-packages\\accelerate\\accelerator.py:1030: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  model.forward = torch.cuda.amp.autocast(dtype=torch.float16)(model.forward)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "#### TODO: gradient_accumulation (optional)####\n",
    "# Note: train_batch_size * gradient_accumulation_steps = effective batch size\n",
    "# If CUDA out of memory, you can make train_batch_size lower and gradient_accumulation_steps upper\n",
    "# Doc: https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation\n",
    "gradient_accumulation_steps = 16\n",
    "optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# dataloader\n",
    "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
    "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
    "train_loader = DataLoader(train_set, batch_size=config.train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "# Change \"fp16_training\" to True to support automatic mixed \n",
    "# precision training (fp16)\t\n",
    "fp16_training = True\n",
    "if fp16_training:    \n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "else:\n",
    "    accelerator = Accelerator()\n",
    "if config.use_wandb:\n",
    "    wandb.init(project=\"Fintuning-QA-Bert\", name=Path(config.savedir).stem, config=config)\n",
    "\n",
    "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9e31e-df52-4f96-95ef-d3a268eda6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c7e5b534dc4990be5eec9da139ee4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 100 | loss = 3.756, acc = 0.080\n",
      "Epoch 1 | Step 200 | loss = 2.002, acc = 0.306\n",
      "Epoch 1 | Step 300 | loss = 1.621, acc = 0.435\n",
      "Epoch 1 | Step 400 | loss = 1.411, acc = 0.481\n",
      "Epoch 1 | Step 500 | loss = 1.206, acc = 0.562\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(config.num_epoch):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "    \n",
    "    for data in tqdm(train_loader):\t\n",
    "        # Load all data into GPU\n",
    "        data = [i.to(device) for i in data]\n",
    "        \n",
    "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
    "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
    "        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "        # Choose the most probable start position / end position\n",
    "        start_index = torch.argmax(output.start_logits, dim=1)\n",
    "        end_index = torch.argmax(output.end_logits, dim=1)\n",
    "        \n",
    "        # Prediction is correct only if both start_index and end_index are correct\n",
    "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "           \n",
    "        train_loss += output.loss\n",
    "        \n",
    "        accelerator.backward(output.loss)\n",
    "        \n",
    "        step += 1\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ##### TODO: Apply linear learning rate decay #####\n",
    "\n",
    "        # Print training loss and accuracy over past logging step\n",
    "        if step % config.logging_step == 0:\n",
    "             if config.use_wandb:\n",
    "                 wandb.log({\n",
    "                    \"train/loss\": train_loss.item() / config.logging_step,\n",
    "                    \"train/acc\": train_acc / config.logging_step,\n",
    "                })\n",
    "             print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / config.logging_step:.3f}, acc = {train_acc / config.logging_step:.3f}\")\n",
    "             train_loss = train_acc = 0\n",
    "\n",
    "    if config.validation:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                # prediction is correct only if answer text exactly matches\n",
    "                dev_acc += evaluate(data, output) == dev_questions[i][\"answer_text\"]\n",
    "            if config.use_wandb: \n",
    "                wandb.log({\n",
    "                    \"Validation/acc\": dev_acc / len(dev_loader),\n",
    "                })\n",
    "            print(f\"Validation | Epoch {config.num_epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "        model.train()\n",
    "\n",
    "# Save a model and its configuration file to the directory 「saved_model」 \n",
    "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
    "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n",
    "print(\"Saving Model ...\")\n",
    "model_save_dir = \"saved_model\" \n",
    "model.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de8146-e91b-4b4d-8b95-b196bb7aa2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Test Set ...\")\n",
    "\n",
    "result = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "        result.append(evaluate(data, output))\n",
    "\n",
    "result_file = \"result.csv\"\n",
    "with open(result_file, 'w') as f:\t\n",
    "    f.write(\"ID,Answer\\n\")\n",
    "    for i, test_question in enumerate(test_questions):\n",
    "    # Replace commas in answers with empty strings (since csv is separated by comma)\n",
    "    # Answers in kaggle are processed in the same way\n",
    "        f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
    "\n",
    "print(f\"Completed! Result is in {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacd9aa-c83d-4db5-8578-ff6b9594c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import random  \n",
    "# import numpy as np\n",
    "\n",
    "# # To avoid CUDA_OUT_OF_MEMORY\n",
    "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "# # Fix random seed for reproducibility\n",
    "# def same_seeds(seed):\n",
    "# \ttorch.manual_seed(seed)\n",
    "# \tif torch.cuda.is_available():\n",
    "# \t\t\ttorch.cuda.manual_seed(seed)\n",
    "# \t\t\ttorch.cuda.manual_seed_all(seed)\n",
    "# \tnp.random.seed(seed)\n",
    "# \trandom.seed(seed)\n",
    "# \ttorch.backends.cudnn.benchmark = False\n",
    "# \ttorch.backends.cudnn.deterministic = True\n",
    "# same_seeds(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b2502-66f6-40a1-86e1-3ebd303ae6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # You can try model with different size\n",
    "# # When using Colab or Kaggle, models with more than 2 billions parameters may \n",
    "# # run out of memory\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-1.7B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ddf22f-f16b-4502-965c-b541d1e582f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To clean model output. If you try different prompts, you may have to fix \n",
    "# # this function on your own\n",
    "# def clean_text(text):\n",
    "#     # Note: When you use unilingual model, the colon may become fullwidth\n",
    "#     text = text.split(\"答案:\")[-1]\n",
    "#     text = text.split(\" \")[0]\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f637b9e-918f-47eb-96ac-b827f70c068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import json\n",
    "\n",
    "# with open(\"hw7_in-context-learning-examples.json\", \"r\") as f: \n",
    "#     test = json.load(f)\n",
    "\n",
    "# # K-shot learning \n",
    "# # Give model K examples to make it achieve better accuracy \n",
    "# # Note: (1) When K >= 4, CUDA_OUT_OFF_MEMORY may occur.\n",
    "# #       (2) The maximum input length of XGLM is 2048\n",
    "# K = 2\n",
    "\n",
    "# question_ids = [qa[\"id\"] for qa in test[\"questions\"]]\n",
    "\n",
    "# with open(\"in-context-learning-result.txt\", \"w\") as f:\n",
    "#     print(\"ID,Ground-Truth,Prediction\", file = f)\n",
    "#     with torch.no_grad():\n",
    "#         for idx, qa in enumerate(test[\"questions\"]):\n",
    "#             # You can try different prompts\n",
    "#             prompt = \"請從最後一篇的文章中找出最後一個問題的答案\\n\"\n",
    "#             exist_question_indexs = [question_ids.index(qa[\"id\"])]\n",
    "\n",
    "#             # K-shot learning: give the model K examples with answers\n",
    "#             for i in range(K):\n",
    "#                 question_index = question_ids.index(qa[\"id\"])\n",
    "#                 while(question_index in exist_question_indexs): \n",
    "#                     question_index = random.randint(0, len(question_ids) - 1)\n",
    "#                 exist_question_indexs.append(question_index)    \n",
    "#                 paragraph_id = test[\"questions\"][question_index][\"paragraph_id\"]\n",
    "#                 prompt += f'文章：{test[\"paragraphs\"][paragraph_id]}\\n'\n",
    "#                 prompt += f'問題：{test[\"questions\"][question_index][\"question_text\"]}\\n'\n",
    "#                 prompt += f'答案：{test[\"questions\"][question_index][\"answer_text\"]}\\n'\n",
    "\n",
    "#             # The final one question without answer\n",
    "#             paragraph_id = qa[\"paragraph_id\"]\n",
    "#             prompt += f'文章：{test[\"paragraphs\"][paragraph_id]}\\n'\n",
    "#             prompt += f'問題：{qa[\"question_text\"]}\\n'\n",
    "#             prompt += f'答案：'\n",
    "            \n",
    "#             inputs = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\") \n",
    "#             sample = model.generate(**inputs, max_new_tokens = 20)\n",
    "#             text = tokenizer.decode(sample[0], skip_special_tokens=True)\n",
    "\n",
    "#             # Note: You can delete this line to see what will happen\n",
    "#             text = clean_text(text)\n",
    "            \n",
    "#             print(prompt)\n",
    "#             print(f'正確答案: {qa[\"answer_text\"]}')\n",
    "#             print(f'模型輸出: {text}')\n",
    "#             print()\n",
    "\n",
    "#             print(f\"{idx},{qa['answer_text']},{text}\", file = f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
