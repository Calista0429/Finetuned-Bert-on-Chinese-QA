{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd857246-0c56-417d-9199-6026af46abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from transformers import AdamW\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c0fa0a-e219-4c31-ab4e-1d817dfa79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "\ttorch.manual_seed(seed)\n",
    "\tif torch.cuda.is_available():\n",
    "\t\t\ttorch.cuda.manual_seed(seed)\n",
    "\t\t\ttorch.cuda.manual_seed_all(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "same_seeds(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4ebdc0-6f70-4b56-8928-e43c68527a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForQuestionAnswering,\n",
    ")\n",
    "\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-chinese\").to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"luhua/chinese_pretrain_mrc_macbert_large\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"luhua/chinese_pretrain_mrc_macbert_large\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")\n",
    "\n",
    "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6c18e1-ac66-483a-b8a3-c062d634552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data[\"questions\"], data[\"paragraphs\"]\n",
    "\n",
    "train_questions, train_paragraphs = read_data(\"D://Code/Machine Translation/hw7_train.json\")\n",
    "dev_questions, dev_paragraphs = read_data(\"D://Code/Machine Translation/hw7_dev.json\")\n",
    "test_questions, test_paragraphs = read_data(\"D://Code/Machine Translation/hw7_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6621b0-6e24-400e-9503-f95efc050dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize questions and paragraphs separately\n",
    "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n",
    "\n",
    "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
    "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
    "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
    "\n",
    "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
    "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
    "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n",
    "\n",
    "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f0297b-7f4a-40c7-ba6b-8205fac6bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 60\n",
    "        self.max_paragraph_len = 150\n",
    "        \n",
    "        ##### TODO: Change value of doc_stride #####\n",
    "        # self.doc_stride = 150\n",
    "        self.doc_stride = 75\n",
    "\n",
    "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
    "\n",
    "        ##### TODO: Preprocessing #####\n",
    "        # Hint: How to prevent model from learning something it should not learn\n",
    "        if self.split == \"train\":\n",
    "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
    "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
    "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
    "\n",
    "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
    "            mid = (answer_start_token + answer_end_token) // 2\n",
    "            \n",
    "            # paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "            # paragraph_end = paragraph_start + self.max_paragraph_len\n",
    "\n",
    "            max_offset = self.max_paragraph_len / 4   # We allow up to 1/4 of the max length as offset\n",
    "            random_offset = np.random.randint(-max_offset, max_offset)  # Random shift between -max_offset and +max_offset\n",
    "\n",
    "            # Adjust paragraph start based on random offset\n",
    "            paragraph_start = max(0, min(mid + random_offset - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "            paragraph_end = paragraph_start + self.max_paragraph_len\n",
    "\n",
    "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
    "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n",
    "            \n",
    "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
    "            answer_start_token += len(input_ids_question) - paragraph_start\n",
    "            answer_end_token += len(input_ids_question) - paragraph_start\n",
    "            \n",
    "            # Pad sequence and obtain inputs to model \n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "            \n",
    "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "                \n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "                \n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "                \n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            \n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
    "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a54b02f-d72f-4a07-8ea4-bdbe2522a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, output):\n",
    "    ##### TODO: Postprocessing #####\n",
    "    # There is a bug and room for improvement in postprocessing \n",
    "    # Hint: Open your prediction file to see what is wrong \n",
    "    \n",
    "    answer = ''\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1]\n",
    "    \n",
    "    for k in range(num_of_windows):\n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
    "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
    "        \n",
    "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
    "        prob = start_prob + end_prob\n",
    "        \n",
    "        # Replace answer if calculated probability is larger than previous windows\n",
    "        #fix the bug \n",
    "        if start_index <= end_index:\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
    "                answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
    "        \n",
    "    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n",
    "    return answer.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b838d1-3806-4520-902e-62bb8c495abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    #save the checkpoint in wandb\n",
    "    savedir = \"./checkpoints/bert-boss\",\n",
    "\n",
    "    # training epoch\n",
    "    num_epoch = 3,\n",
    "    \n",
    "    #Validate on validation set\n",
    "    validation = True,\n",
    "    \n",
    "    #print the loss per logging step\n",
    "    logging_step = 100,\n",
    "    \n",
    "    #learning rate\n",
    "    learning_rate = 1e-5,\n",
    "\n",
    "    # training batch size \n",
    "    train_batch_size = 8,\n",
    "\n",
    "    # logging\n",
    "    use_wandb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75ebc212-3ebc-4028-8bd4-04ead52d9889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: baoxihuang0429 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Code\\Machine Translation\\wandb\\run-20250404_143544-mmmusg1b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert/runs/mmmusg1b' target=\"_blank\">bert-boss</a></strong> to <a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert' target=\"_blank\">https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert/runs/mmmusg1b' target=\"_blank\">https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert/runs/mmmusg1b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#### TODO: gradient_accumulation (optional)####\n",
    "# Note: train_batch_size * gradient_accumulation_steps = effective batch size\n",
    "# If CUDA out of memory, you can make train_batch_size lower and gradient_accumulation_steps upper\n",
    "# Doc: https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation\n",
    "gradient_accumulation_steps = 16\n",
    "optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# dataloader\n",
    "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
    "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
    "train_loader = DataLoader(train_set, batch_size=config.train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "total_steps = len(train_loader) * config.num_epoch\n",
    "num_warmup_steps = int(0.2 * total_steps)  # Set warmup steps to 20% of total steps\n",
    "# [Hugging Face] Apply linear learning rate decay with warmup\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Change \"fp16_training\" to True to support automatic mixed \n",
    "# precision training (fp16)\t\n",
    "fp16_training = True\n",
    "if fp16_training:    \n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "else:\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "if config.use_wandb:\n",
    "    wandb.init(project=\"Fintuning-QA-Bert\", name=Path(config.savedir).stem, config=config)\n",
    "\n",
    "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/\n",
    "model, optimizer, train_loader, scheduler = accelerator.prepare(model, optimizer, train_loader, scheduler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9e31e-df52-4f96-95ef-d3a268eda6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd64ba074fe47bba7ebeaed89f4b892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 100 | loss = 2.334, acc = 0.411\n",
      "Epoch 1 | Step 200 | loss = 2.113, acc = 0.445\n",
      "Epoch 1 | Step 300 | loss = 1.698, acc = 0.519\n",
      "Epoch 1 | Step 400 | loss = 1.320, acc = 0.592\n",
      "Epoch 1 | Step 500 | loss = 1.182, acc = 0.610\n",
      "Epoch 1 | Step 600 | loss = 1.047, acc = 0.634\n",
      "Epoch 1 | Step 700 | loss = 0.910, acc = 0.660\n",
      "Epoch 1 | Step 800 | loss = 0.911, acc = 0.642\n",
      "Epoch 1 | Step 900 | loss = 0.796, acc = 0.681\n",
      "Epoch 1 | Step 1000 | loss = 0.704, acc = 0.709\n",
      "Epoch 1 | Step 1100 | loss = 0.636, acc = 0.745\n",
      "Epoch 1 | Step 1200 | loss = 0.692, acc = 0.725\n",
      "Epoch 1 | Step 1300 | loss = 0.619, acc = 0.738\n",
      "Epoch 1 | Step 1400 | loss = 0.625, acc = 0.729\n",
      "Epoch 1 | Step 1500 | loss = 0.689, acc = 0.724\n",
      "Epoch 1 | Step 1600 | loss = 0.568, acc = 0.759\n",
      "Epoch 1 | Step 1700 | loss = 0.604, acc = 0.759\n",
      "Epoch 1 | Step 1800 | loss = 0.588, acc = 0.743\n",
      "Epoch 1 | Step 1900 | loss = 0.549, acc = 0.752\n",
      "Epoch 1 | Step 2000 | loss = 0.724, acc = 0.721\n",
      "Epoch 1 | Step 2100 | loss = 0.588, acc = 0.745\n",
      "Epoch 1 | Step 2200 | loss = 0.557, acc = 0.771\n",
      "Epoch 1 | Step 2300 | loss = 0.591, acc = 0.745\n",
      "Epoch 1 | Step 2400 | loss = 0.549, acc = 0.750\n",
      "Epoch 1 | Step 2500 | loss = 0.590, acc = 0.746\n",
      "Epoch 1 | Step 2600 | loss = 0.580, acc = 0.767\n",
      "Epoch 1 | Step 2700 | loss = 0.580, acc = 0.757\n",
      "Epoch 1 | Step 2800 | loss = 0.533, acc = 0.791\n",
      "Epoch 1 | Step 2900 | loss = 0.524, acc = 0.779\n",
      "Epoch 1 | Step 3000 | loss = 0.555, acc = 0.766\n",
      "Epoch 1 | Step 3100 | loss = 0.579, acc = 0.769\n",
      "Epoch 1 | Step 3200 | loss = 0.613, acc = 0.767\n",
      "Epoch 1 | Step 3300 | loss = 0.550, acc = 0.760\n",
      "Evaluating Dev Set ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44ccabb9055478b85b457aef0dcf4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2863 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Epoch 4 | acc = 0.785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f660d663bcb0411c80c5097f6b0be383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 100 | loss = 0.481, acc = 0.765\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(config.num_epoch):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "    \n",
    "    for data in tqdm(train_loader):\t\n",
    "        with accelerator.accumulate(model):\n",
    "            \n",
    "            # Load all data into GPU\n",
    "            data = [i.to(device) for i in data]\n",
    "            \n",
    "            # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
    "            # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
    "            output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "            # Choose the most probable start position / end position\n",
    "            start_index = torch.argmax(output.start_logits, dim=1)\n",
    "            end_index = torch.argmax(output.end_logits, dim=1)\n",
    "            \n",
    "            # Prediction is correct only if both start_index and end_index are correct\n",
    "            train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "               \n",
    "            train_loss += output.loss\n",
    "            \n",
    "            accelerator.backward(output.loss)\n",
    "            \n",
    "            step += 1\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        ##### TODO: Apply linear learning rate decay #####\n",
    "\n",
    "        # Print training loss and accuracy over past logging step\n",
    "        if step % config.logging_step == 0:\n",
    "             if config.use_wandb:\n",
    "                 wandb.log({\n",
    "                    \"train/loss\": train_loss.item() / config.logging_step,\n",
    "                    \"train/acc\": train_acc / config.logging_step,\n",
    "                })\n",
    "             print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / config.logging_step:.3f}, acc = {train_acc / config.logging_step:.3f}\")\n",
    "             train_loss = train_acc = 0\n",
    "\n",
    "    if config.validation:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                # prediction is correct only if answer text exactly matches\n",
    "                dev_acc += evaluate(data, output) == dev_questions[i][\"answer_text\"]\n",
    "            if config.use_wandb: \n",
    "                wandb.log({\n",
    "                    \"Validation/acc\": dev_acc / len(dev_loader),\n",
    "                })\n",
    "            print(f\"Validation | Epoch {config.num_epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "        model.train()\n",
    "\n",
    "# Save a model and its configuration file to the directory 「saved_model」 \n",
    "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
    "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n",
    "print(\"Saving Model ...\")\n",
    "model_save_dir = \"saved_model\" \n",
    "model.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77f6f1ea-afa6-4236-9d17-32652848fdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation/acc</td><td>▁▇█</td></tr><tr><td>train/acc</td><td>▁▁▁▁▁▁▁▂▅▅▆▆▆▇▆▇▇▇▇▇▇▇▇█▇███████▇██████▇</td></tr><tr><td>train/loss</td><td>███▇▇▆▆▄▄▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation/acc</td><td>0.69787</td></tr><tr><td>train/acc</td><td>0.63</td></tr><tr><td>train/loss</td><td>0.85947</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert-simple</strong> at: <a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert/runs/54c55v5x' target=\"_blank\">https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert/runs/54c55v5x</a><br> View project at: <a href='https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert' target=\"_blank\">https://wandb.ai/baoxihuang0429/Fintuning-QA-Bert</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250402_171739-54c55v5x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b79f35-5e83-420e-835b-73f9d4321655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
